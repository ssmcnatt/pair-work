{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gSTzCDSCgeC"
   },
   "source": [
    "# Text Vectorization and Feature Engineering Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "gunVMV4FCgeF"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "    print(\"Corpus Statistics\")\n",
    "    print(\"Number of documents: \" + str(len(corpus.fileids())))\n",
    "    print(\"Number of paragraphs: \" + str(len(corpus.paras())))\n",
    "    print(\"Number of sentences: \" + str(len(corpus.sents())))\n",
    "    print(\"Number of words: \" + str(len(corpus.words())))\n",
    "    print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "    print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "    print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tra0ZNCICgeI"
   },
   "source": [
    "### Read the CNN Lite plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "iVzAYR4CCgeJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics\n",
      "Number of documents: 14\n",
      "Number of paragraphs: 14\n",
      "Number of sentences: 427\n",
      "Number of words: 13668\n",
      "Vocabulary: 2927\n",
      "Avg chars per word: 5.0\n",
      "Avg words per sentence: 32.0\n"
     ]
    }
   ],
   "source": [
    "PATH = 'lite_cnn/'\n",
    "DOC_PATTERN = r'articles_text.*\\.p'\n",
    "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\n",
    "corpus_stats(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BmAR8xjPCgeM"
   },
   "source": [
    "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "e9k2CshRCgeM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pink taking a break to focus on family(CNN) - Pink has been working pretty hard and it sounds like she will be taking a step back in 2020.Speaking with \"Entertainment Tonight\" on the Country Music Association Awards red carpet, the singer was joined by her husband, Carey Hart, and their kids Willow, 8, and Jameson, 2.Pink was there to perform her song \"Love Me Anyway\" with country star Chris Stapleton, and she  talked about how hectic things have been. \"We did two and a half years of [music] and Willow\\'s back in school now, Jameson\\'s going to start pre-school soon,\" Pink said. \"It\\'s kind of the year of the family.\"The star also praised her husband, with whom she will celebrate 14 years of marriage in January.\"Carey has a lot going on as well,\" she said of Hart, who went from being a professional motocross competitor to racing off-road trucks. \"He\\'s super supportive, he follows me around the world and now it\\'s his turn.\"According to Billboard, Pink\\'s Beautiful Trauma Tour ranks as the 10th highest-grossing tour of all time, earning more than $397 million. '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = []\n",
    "\n",
    "for fileid in corpus.fileids():\n",
    "    doc = corpus.raw(fileid)\n",
    "    docs.append(doc)\n",
    "    \n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhj0bcsqCgeO"
   },
   "source": [
    "### Preprocess and clean the documents according to the steps below.\n",
    "\n",
    "- Word Tokenize\n",
    "- Lowercase\n",
    "- Remove Stopwords\n",
    "- Remove Punctuation\n",
    "- Lemmatize\n",
    "- Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-_-hyZd6CgeP",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_words = [word_tokenize(doc) for doc in docs]\n",
    "doc_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0038a4e1c49f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords_lower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-9-0038a4e1c49f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwords_lower\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "doc_words_lower = []\n",
    "for doc in doc_words:\n",
    "    doc_words_lower.append([word.lower() for word in doc_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ws4b33ZUCgeR"
   },
   "source": [
    "### Count vectorize the preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LUrhKyACgeS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zs5UbfvBCgeU"
   },
   "source": [
    "### One hot vectorize the preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TTwAIFI5CgeU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HsUHIRZnCgeW"
   },
   "source": [
    "### TF-IDF vectorize the preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN10u75HCgeX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tyf1oG4CgeZ"
   },
   "source": [
    "### Use Doc2Vec to vectorize the preprocessed documents.\n",
    "\n",
    "Set the size of the vectors to be the same size as those of the other methods using the `vector_size` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4HgOxv6CgeZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Day 72, Lecture 2: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
