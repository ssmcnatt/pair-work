{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E0voemCtIvus"
   },
   "source": [
    "# Extracting Information from Text Data Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!pip install textacy --quiet\n",
    "#!pip install rake_nltk --quiet\n",
    "#!conda install -c conda-forge textacy\n",
    "#!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QNIoK-wzIvuv"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import string\n",
    "import textacy\n",
    "import itertools\n",
    "from nltk import pos_tag\n",
    "from rake_nltk import Rake\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import tree2conlltags\n",
    "from gensim.summarization import keywords\n",
    "from nltk.chunk.regexp import RegexpParser\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EbB__zIWzob1"
   },
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "    print(\"Corpus Statistics\")\n",
    "    print(\"Number of documents: \" + str(len(corpus.fileids())))\n",
    "    print(\"Number of paragraphs: \" + str(len(corpus.paras())))\n",
    "    print(\"Number of sentences: \" + str(len(corpus.sents())))\n",
    "    print(\"Number of words: \" + str(len(corpus.words())))\n",
    "    print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "    print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "    print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4M7RrWRKIvuy"
   },
   "source": [
    "### Read the CNN Lite plain text file articles into a corpus using the NLTK's PlaintextCorpusReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gMwqpeAmIvuz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus Statistics\n",
      "Number of documents: 14\n",
      "Number of paragraphs: 14\n",
      "Number of sentences: 427\n",
      "Number of words: 13668\n",
      "Vocabulary: 2927\n",
      "Avg chars per word: 5.0\n",
      "Avg words per sentence: 32.0\n"
     ]
    }
   ],
   "source": [
    "PATH = 'lite_cnn/'\n",
    "DOC_PATTERN = r'articles_text.*\\.p'\n",
    "corpus = PlaintextCorpusReader(PATH, DOC_PATTERN)\n",
    "corpus_stats(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y6Zp26sVIvu1"
   },
   "source": [
    "### Iterate through the fileids in the corpus, extract the raw text of each document, and store them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "yl15hykfIvu2"
   },
   "outputs": [],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5LLM0YPIvu4"
   },
   "source": [
    "### Extract the top 5 keywords from every document in the corpus. Print them and compare the differences in keywords among the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wcXEjv6FIvu4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pink', 'carey', 'tour', 'year', 'pretty']\n",
      "['patrick', 'primary', 'telling', 'democrats', 'cnn']\n",
      "['narwhal', 'tail', 'puppy', 'unicorn', 'dogs']\n",
      "['states', 'democratic', 'bloomberg', 'told', 'running']\n",
      "['republican', 'taylor', 'rep', 'presidents', 'ukraine']\n",
      "['muslimness', 'people', 'skin', 'white', 'religion']\n",
      "['trump', 'new', 'said', 'republican', 'media']\n",
      "['said', 'police', 'brown', 'jones', 'roanoke']\n",
      "['trump', 'hotels', 'office', 'profit', 'owned']\n",
      "['keys', 'grammys', 'award', 'alicia', 'power']\n",
      "['americans', 'republican', 'trump', 'ukrainians', 'investigate']\n",
      "['student', 'said', 'told', 'pence', 'schools']\n",
      "['crows', 'disney', 'american', 'old', 'movie']\n",
      "['protester', 'police', 'chinese', 'new', 'kong']\n"
     ]
    }
   ],
   "source": [
    "for doc in docs:\n",
    "    keys = keywords(doc, words=5, lemmatize=True, split=True)\n",
    "    print(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6w50BcwMIvu6"
   },
   "source": [
    "### Extract the top 3 keyphrases from each document, print them, and compare the differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "7M0PDiFwIvu7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(32.5, 'country music association awards red carpet'), (17.166666666666664, 'school soon ,\" pink said'), (16.5, 'country star chris stapleton')]\n",
      "[(94.03333333333333, 'former new york city mayor michael bloomberg stepped forward last week'), (30.61111111111111, 'patrick could seize upon potential advantages'), (29.0, 'elections process would ultimately splash back')]\n",
      "[(28.166666666666668, 'little magical furry unicorn ,\" according'), (17.666666666666664, 'dog rescue nonprofit organization mac'), (13.666666666666666, 'rescue workers speculate may')]\n",
      "[(40.45, 'current 2020 democrats -- clinton told bbc radio'), (37.666666666666664, 'former new york mayor michael bloomberg made'), (29.0, '2016 democratic nominee playfully tweeted back')]\n",
      "[(69.41666666666667, 'money ,\" tweeted white house press secretary stephanie grisham'), (67.75, 'former white house homeland security adviser tom bossert summed'), (63.416666666666664, 'former national security council russia expert fiona hill said')]\n",
      "[(20.77777777777778, 'support various international policies --'), (16.0, 'protesters stormed john f'), (16.0, 'executive order banning entry')]\n",
      "[(78.26923076923076, 'others made jokes â€” christian whiton said witnesses bill taylor'), (46.333333333333336, 'heard white house press secretary stephanie grisham say'), (42.65, 'hour ... ed henry shares hearsay laura ingraham')]\n",
      "[(34.266666666666666, 'police saythough franklin county authorities said earlier'), (23.975, 'roanoke police chief tim jones said'), (20.55833333333333, 'capacity weapon ,\" jones said')]\n",
      "[(30.03333333333333, ',\" said committee chairman peter defazio'), (23.5, 'wall street journal first reported'), (23.0, 'hired real estate firm jll')]\n",
      "[(19.333333333333336, 'host ,\" keys told billboard'), (18.166666666666668, 'current alicia keys discussing hosting'), (16.0, 'regular award receiver helped')]\n",
      "[(32.6, 'house intelligence committee chair adam schiff'), (27.916666666666664, 'republicans like jim jordan threw angrily'), (23.75, 'courts also prohibit witness speculation')]\n",
      "[(60.595238095238095, 'posted threats please notify us asap ,\" murakami wrote'), (51.166666666666664, 'white house sends condolencesvice president mike pence gave'), (34.31666666666666, 'los angeles county sheriff alex villanueva said')]\n",
      "[(22.000000000000004, 'may contain outdated cultural depictions'), (21.666666666666664, \"outdated cultural depictions '( cnn\"), (16.0, 'director tim burton chose')]\n",
      "[(31.5, 'fall semester around two weeks early'), (30.955555555555556, 'senior chinese official shen chunyao said'), (29.569444444444443, 'three tunnels connecting hong kong island')]\n"
     ]
    }
   ],
   "source": [
    "r = Rake()\n",
    "for doc in docs:\n",
    "    key_phrases = r.extract_keywords_from_text(doc)\n",
    "    key_phrases = r.get_ranked_phrases_with_scores()\n",
    "    print(key_phrases[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rev-FI9HIvu9"
   },
   "source": [
    "### Identify and extract the named entities in each document, filtering out the numeric types. Print them and compare the differences between documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "TLs9TLMHIvu9"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Entertainment Tonight', 'WORK_OF_ART'],\n",
       " ['the Country Music Association Awards', 'ORG'],\n",
       " ['Carey Hart', 'PERSON'],\n",
       " ['Willow', 'PERSON'],\n",
       " ['8', 'DATE'],\n",
       " ['Jameson', 'PERSON'],\n",
       " ['2.Pink', 'CARDINAL'],\n",
       " ['Love Me Anyway', 'WORK_OF_ART'],\n",
       " ['Chris Stapleton', 'PERSON'],\n",
       " ['two and a half years', 'DATE'],\n",
       " ['Willow', 'PERSON'],\n",
       " ['Jameson', 'PERSON'],\n",
       " ['Pink', 'ORG'],\n",
       " ['the year', 'DATE'],\n",
       " ['14 years', 'DATE'],\n",
       " ['January', 'DATE'],\n",
       " ['\"Carey', 'ORG'],\n",
       " ['Hart', 'PERSON'],\n",
       " ['10th', 'ORDINAL'],\n",
       " ['more than $397 million', 'MONEY']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = []\n",
    "for doc in docs:\n",
    "    spacy_doc = nlp(doc)\n",
    "    entities.append([[entity.text, entity.label_] for entity in spacy_doc.ents])\n",
    "\n",
    "entities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UwhH404sIvvB"
   },
   "source": [
    "### For every document in the corpus, iterate over every sentence, extract any SVO triples, print them, and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03XLMyk1IvvC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Natural Language Processing Day 3 Morning Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
