{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4I54bZzMImI"
   },
   "source": [
    "# Machine Learning: Text Classification Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "NBhHdzKtMImK"
   },
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs): \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmer = SnowballStemmer('english') \n",
    "    \n",
    "    preprocessed = []\n",
    "    for doc in docs: \n",
    "        tokenized = word_tokenize(doc)\n",
    "        cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) for token in tokenized \n",
    "               if not token.lower() in stopwords.words('english') \n",
    "               if token.isalpha()]\n",
    "\n",
    "        untokenized = \" \".join(cleaned)\n",
    "        preprocessed.append(untokenized)\n",
    "        \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d2tVDJJaMImN"
   },
   "source": [
    "### Use the CategorizedPlaintextCorpusReader to import the AP_News corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "REtoZb_iMImO"
   },
   "outputs": [],
   "source": [
    "path = 'AP_News'\n",
    "DOC_PATTERN = r'.*\\.txt'\n",
    "CAT_PATTERN = r'([\\w_\\s]+)/.*'\n",
    "\n",
    "corpus = CategorizedPlaintextCorpusReader(path, DOC_PATTERN, cat_pattern=CAT_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SQMuBvquMImP"
   },
   "source": [
    "### Create two separate lists - one containing the text from each document and another containing the category of each article in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8jU4ZNM-MImQ"
   },
   "outputs": [],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]\n",
    "\n",
    "categories = [corpus.categories(fileid)[0] for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGRABGW8MImR"
   },
   "source": [
    "### Preprocess the corpus, ensuring to include the following steps.\n",
    "\n",
    "- Word tokenize the documents.\n",
    "- Lemmatize, stem, and lowercase all tokens.\n",
    "- Remove punctuation and stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hk-Nlze1MImS"
   },
   "outputs": [],
   "source": [
    "preprocessed = preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZE-q4ziMImT"
   },
   "source": [
    "### Split the data into training and testing sets with the size of the test set being 30% of the records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "hKFyBgjBMImU"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(preprocessed, categories, test_size=0.30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "reegQu_5MImV"
   },
   "source": [
    "### Construct a pipeline that TF-IDF vectorizes the text and trains a Random Forest classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "_3JJ3hjNMImW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='auto', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Pipeline([\n",
    "            ('vect', CountVectorizer()),\n",
    "            ('tfidf', TfidfTransformer()),\n",
    "            ('clf', LogisticRegression()),\n",
    "])\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFt6djjpMImX"
   },
   "source": [
    "### Generate predictions on the test set and print a classification report to evaluate how well the model performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "yWT99tvHMImY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      health       0.81      0.59      0.68        22\n",
      "    politics       0.86      0.75      0.80        16\n",
      "      sports       1.00      0.76      0.87        17\n",
      "        tech       0.39      0.82      0.53        11\n",
      "\n",
      "    accuracy                           0.71        66\n",
      "   macro avg       0.77      0.73      0.72        66\n",
      "weighted avg       0.80      0.71      0.73        66\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2BwKeBNMImZ"
   },
   "source": [
    "### Perform 10-fold cross validation and obtain the averge F1 score across all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "-n9cTvc6MIma"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8187741262005968"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_val_score(model, preprocessed, categories,\n",
    "                         cv=10, scoring = 'f1_macro')\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H9ERShiMImb"
   },
   "source": [
    "### Ingest, preprocess, and predict the topic of the article at the following URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0d2mfYGSMImc"
   },
   "outputs": [],
   "source": [
    "url = 'https://www.nytimes.com/2019/11/25/business/uber-london.html'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "7piaPcKNMImd"
   },
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "content = response.text\n",
    "\n",
    "tags = ['p', 'li']\n",
    "soup = BeautifulSoup(content, 'lxml')\n",
    "text_list = [tag.get_text() for tag in soup.find_all(tags)]\n",
    "text = ' '.join(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tech'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned = preprocess([text])\n",
    "model.predict(cleaned)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 74, Lecture 1: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
