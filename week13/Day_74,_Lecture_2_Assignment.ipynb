{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpAM5or7MJg8"
   },
   "source": [
    "# Machine Learning on Text: Clustering Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Nw7-5JgzMJg9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.cluster import KElbowVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(docs): \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    stemmer = SnowballStemmer('english') \n",
    "    \n",
    "    preprocessed = []\n",
    "    for doc in docs: \n",
    "        tokenized = word_tokenize(doc)\n",
    "        cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) for token in tokenized \n",
    "               if not token.lower() in stopwords.words('english') \n",
    "               if token.isalpha()]\n",
    "\n",
    "        untokenized = \" \".join(cleaned)\n",
    "        preprocessed.append(untokenized)\n",
    "        \n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7IzBzLsMJhA"
   },
   "source": [
    "### Ingest the company_profiles data set into a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "u-u8MsfKMJhB"
   },
   "outputs": [],
   "source": [
    "path = 'company_profiles'\n",
    "DOC_PATTERN = r'.*\\.txt'\n",
    "\n",
    "corpus = PlaintextCorpusReader(path, DOC_PATTERN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evt_VveIMJhD"
   },
   "source": [
    "### Create a list of documents by extracting the raw text for each fileid in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "0Q0T6d2TMJhD"
   },
   "outputs": [],
   "source": [
    "docs = [corpus.raw(fileid) for fileid in corpus.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L98OWhMMJhG"
   },
   "source": [
    "### Preprocess the documents, including the steps below.\n",
    "\n",
    "- Word tokenize the document.\n",
    "- Lowercase all tokens.\n",
    "- Lemmatize and stem the tokens.\n",
    "- Remove stop words and punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XJGYG2L_MJhG"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed = preprocess(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sw3HvSNuMJhI"
   },
   "source": [
    "### TF-IDF vectorize the preprocessed documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYFXymYtMJhJ"
   },
   "outputs": [],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4F3w9-n7MJhL"
   },
   "source": [
    "### Determine the optimal number of clusters using the Yellowbrick library's KElbow Visualizer and a KMeans clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDBdUa03MJhM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QIurKlo-MJhP"
   },
   "source": [
    "### Perform K-Means Clustering using the optimal number of clusters determine in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jd0DuxrKMJhP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzlVKvK1MJhR"
   },
   "source": [
    "### Perform Agglomerative Clustering using the same number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXa_ofOOMJhS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFlkG5TyMJhU"
   },
   "source": [
    "### Choose one of the three topic modeling approaches covered. Cluster into the optimal number of clusters and extract 5 keywords that represent the documents in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "29jDpqjAMJhV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Day 74, Lecture 2: Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
