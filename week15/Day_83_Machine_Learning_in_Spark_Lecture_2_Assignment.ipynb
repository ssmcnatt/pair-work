{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_83_Machine_Learning_in_Spark_Lecture_2_Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry4VDk_llCLY"
      },
      "source": [
        "## Day 83 Lecture 2 Assignment\n",
        "\n",
        "In this assignment, we will learn about Spark and MLLib.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNByDud4iU40"
      },
      "source": [
        "Update the Google Compute Engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIxqnOXGS1OX"
      },
      "source": [
        "! apt-get update"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XJYeAHeihaf"
      },
      "source": [
        "Run the cells below to start a spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wen6A2I2RYVV"
      },
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.6/spark-2.4.6-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.6-bin-hadoop2.7.tgz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zm3pHHHxNmkW"
      },
      "source": [
        "Set up the environmental variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW40H37fIb0t"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.6-bin-hadoop2.7\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vdAA2aoNqhz"
      },
      "source": [
        "Install Pyspark. ***Note: The version must match the version of Spark that you have installed earlier***."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "274gUgKyIjnC"
      },
      "source": [
        "!pip install -q findspark\n",
        "!pip install pyspark==2.4.6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtsLvt2JlI7X"
      },
      "source": [
        "Download and save the video games sales CSV file to your Colab Data folder on Google Drive. \n",
        "\n",
        "The file can be downloaded from link [here](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Big%20Data/Video_Games_Sales_as_at_22_Dec_2016.csv), download it and save it to your Google Colab Data folder on gdrive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1TXjLAWN3Wd"
      },
      "source": [
        "Mount your Google drive for access in Google Colab Notebooks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFXDtsaOIsLI"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIuuMcVvPH9l"
      },
      "source": [
        "Change your directory to the Colab Data folder where you saved the CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8_ke5oAkBU6"
      },
      "source": [
        "os.chdir('/content/gdrive/My Drive/Colab Data')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hcU6yqpgO_of"
      },
      "source": [
        "Set your local session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRiTkZyDebpj"
      },
      "source": [
        "APP_NAME = \"Day83\"\n",
        "SPARK_URL = \"local[*]\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lkb7U6V5ioAP"
      },
      "source": [
        "Run the cells below to start a spark session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flNZrdlNDuas"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(APP_NAME).getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux-rFpwBimqD"
      },
      "source": [
        "List all the files in your directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEln8x7hx7H-"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yo2CIL4XPRzn"
      },
      "source": [
        "Load the CSV file as a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvrsUPFKUgWJ"
      },
      "source": [
        "video = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Video_Games_Sales_as_at_22_Dec_2016.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1OiHvakqnM1"
      },
      "source": [
        "Review the data by applying the show command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yzV32sgqidJ"
      },
      "source": [
        "# Answer below:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9_58zZnQ4m8"
      },
      "source": [
        "We will predict global sales using a number of variables in this dataset. We will start by removing all missing data (though we know that this will make the dataset significantly smaller). Drop all the missing values using dropna."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnKVtUnoKtS9"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_CgzqoLDw5aU"
      },
      "source": [
        "Get the unique values for Genre.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bhnicFVw8uN"
      },
      "source": [
        "distinct_g = [x.Genre for x in video.select('Genre').distinct().collect()]\n",
        "len(distinct_g)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X06gNwR0V2Q3"
      },
      "source": [
        "Next, we will create dummy variables for the genre. Create these variables using the `OneHotEncoder` provided in spark."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpRz8gMHV1jS"
      },
      "source": [
        "# Answer below:\n",
        "\n",
        "\n",
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "\n",
        "stringIndexer = StringIndexer(inputCol=\"Genre\", outputCol=\"GenreIndex\")\n",
        "model = stringIndexer.fit(video_na)\n",
        "indexed = model.transform(video_na)\n",
        "encoder = OneHotEncoder( inputCol=\"GenreIndex\", outputCol=\"GenreVec\")\n",
        "encoded = encoder.transform(indexed)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_5HOoEsWt96"
      },
      "source": [
        "encoded.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiQXmq_fXntw"
      },
      "source": [
        "Convert the critic score and the user score to a number between 0 and 1 by dividing by 100. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VgfllC7XPKP"
      },
      "source": [
        "# Answer below:\n",
        "\n",
        "from pyspark.sql.functions import lit, col\n",
        "\n",
        "video_percent = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92XImZFDYXnf"
      },
      "source": [
        "Using the vector assembler, create a vector of features using the scaled user score, the scaled critic score and the one hot encoded vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpA7hSQsX5hR"
      },
      "source": [
        "# Answer below:\n",
        "\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cr5N6lMancOl"
      },
      "source": [
        "Split the data into 70% in the training sample and 30% in the test sample."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZg3gX4rni8z"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tK2oIPrFVJ0u"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_mbNCz6njMC"
      },
      "source": [
        "Using the train and test data, generate a linear regression to predict global sales. Print the r squared from the model summary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNRgTZIZm1uI"
      },
      "source": [
        "# Answer below:\n",
        "\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "\n",
        "lr = \n",
        "\n",
        "# Fit the model\n",
        "\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-z7sB1ljDZ7"
      },
      "source": [
        "Calculate the RMSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qo3l9oc8qJDq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0QCZg_uP0CA"
      },
      "source": [
        "## Classification - Binomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T86Pj9Frjyhf"
      },
      "source": [
        "Download and save the college admission  CSV file to your Colab Data folder on Google Drive. \n",
        "\n",
        "The file can be downloaded from link [here](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Admission_Predict.csv), download it and save it to your Google Colab Data folder on gdrive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSyE1P5ikFdp"
      },
      "source": [
        "Load the admit CSV file in to the spark instance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhlRdBi3MYlr"
      },
      "source": [
        "# Answer below:\n",
        "admit = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Admission_Predict.csv\")\n",
        "admit.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqZ4DhjMW6on"
      },
      "source": [
        "admit.printSchema()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5ZzjBv6Smf5"
      },
      "source": [
        "Create a categorical response from `Chance of Admit` column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU1-SgIHWS8b"
      },
      "source": [
        "from pyspark.ml.feature import Binarizer\n",
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3NQ30HKbpqr"
      },
      "source": [
        "Divide GRE and TOEFL score columns by 100 to normalize the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjcYYZ3oU7DY"
      },
      "source": [
        "from pyspark.sql.functions import lit, col\n",
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1cIz6BWc12n"
      },
      "source": [
        "Vectorize the features using the vector assembler, saving them into a column named features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4Ibl9MkcPs4"
      },
      "source": [
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfkkSBzfk93I"
      },
      "source": [
        "Split into train and test subsets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrkM1eCtlBnx"
      },
      "source": [
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3KbBnp4lD1G"
      },
      "source": [
        "Define the Logistic regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LncfrOdzdUbR"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression\n",
        "logr = \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOuEI6XBlLQR"
      },
      "source": [
        "Fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z4oo9o4eMX3"
      },
      "source": [
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-JLjNUmlN-E"
      },
      "source": [
        "predict in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "irZa6ZrCesiG"
      },
      "source": [
        "# Answer below\n",
        "predictions = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8swSBazulUqH"
      },
      "source": [
        "Plot the ROC."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-xtF0dWey07"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "trainingSummary = model.summary\n",
        "roc = trainingSummary.roc.toPandas()\n",
        "plt.plot(roc['FPR'],roc['TPR'])\n",
        "plt.ylabel('False Positive Rate')\n",
        "plt.xlabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.show()\n",
        "print('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvhfWKjTlX-N"
      },
      "source": [
        "Calculate the overall model accuracy using the model evaluator.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMTeVl6kfr_j"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "# Answer below"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwbB3HbXgslz"
      },
      "source": [
        "## Classification - Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WpKBELD3lkmX"
      },
      "source": [
        "Download and save the land cover type CSV file to your Colab Data folder on Google Drive. \n",
        "\n",
        "The file can be downloaded from link [here](https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/Data%20Sets%20Big%20Data/covtype.csv), download it and save it to your Google Colab Data folder on gdrive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJNJBjuDCOCk"
      },
      "source": [
        "Load the land cover type data from this CSV file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkTAb55PgKt-"
      },
      "source": [
        "#Answer below:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EN6ppx1qFINC"
      },
      "source": [
        "#load modules\n",
        "# !!!!caution: not from pyspark.mllib.linalg import Vectors\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import IndexToString,StringIndexer, VectorIndexer\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-IFywYblz5e"
      },
      "source": [
        "Vectorize the features to a single column called features using a lambda function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwmrkR27E47j"
      },
      "source": [
        "#Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKvre3Xy-AL9"
      },
      "source": [
        "# Split the data into training and test sets (40% held out for testing)\n",
        "#Answer below:"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0Sx4U14-Lpg"
      },
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "\n",
        "# Define a RandomForest model.\n",
        "rf = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9at6mz78mK4v"
      },
      "source": [
        "Fit the model and predict on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FhL9nJiIAXG"
      },
      "source": [
        "#Answer below:\n",
        "predictions = \n",
        "predictions.show(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6TyrGJsmULB"
      },
      "source": [
        "Build a classification matrix to review the model preformance with the data test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A3D-XbnTjTNT"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "#Answer below:\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOMVb_TxLIBr"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}