{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Day 79 Lecture 2 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQfS-31Jh-bL"
      },
      "source": [
        "## Recurrent Neural Networks\n",
        "\n",
        "In this assignment, we will learn about recurrent neural networks. We will create an RNN and learn to classify text data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOy993_Xh-bM"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ToNRsZf5h-bP",
        "outputId": "577e6a51-7984-444b-c64f-d01432c0b17f"
      },
      "source": [
        "yelp = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/yelp_labeled.csv', error_bad_lines=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Skipping line 281: expected 2 fields, saw 3\\nSkipping line 290: expected 2 fields, saw 3\\nSkipping line 296: expected 2 fields, saw 3\\nSkipping line 322: expected 2 fields, saw 3\\nSkipping line 373: expected 2 fields, saw 3\\nSkipping line 417: expected 2 fields, saw 3\\nSkipping line 427: expected 2 fields, saw 3\\nSkipping line 429: expected 2 fields, saw 3\\nSkipping line 577: expected 2 fields, saw 3\\nSkipping line 578: expected 2 fields, saw 3\\nSkipping line 611: expected 2 fields, saw 3\\nSkipping line 677: expected 2 fields, saw 3\\nSkipping line 771: expected 2 fields, saw 3\\nSkipping line 930: expected 2 fields, saw 3\\nSkipping line 979: expected 2 fields, saw 4\\nSkipping line 980: expected 2 fields, saw 3\\n'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fKQfObOh-bX",
        "outputId": "9deafdf6-9637-4fea-d013-3519015b3b1c"
      },
      "source": [
        "yelp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Wow... Loved this place.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Crust is not good.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Not tasty and the texture was just nasty.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Stopped by during the late May bank holiday of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The selection on the menu was great and so wer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0                           Wow... Loved this place.          1\n",
              "1                                 Crust is not good.          0\n",
              "2          Not tasty and the texture was just nasty.          0\n",
              "3  Stopped by during the late May bank holiday of...          1\n",
              "4  The selection on the menu was great and so wer...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXyJFyo9h-bZ",
        "outputId": "6b3f7a76-0987-4680-abd2-c182a0339952"
      },
      "source": [
        "yelp.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    494\n",
              "0    482\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp0a4qU9h-bb"
      },
      "source": [
        "We have loaded a Yelp review dataset above. A positive sentiment is classified as 1 and a negative sentiment is classified as 0. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zyboex_Vh-bb"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words)       \n",
        "\n",
        "def stem_list(word_list):\n",
        "    stemmed = []\n",
        "    for word in word_list:\n",
        "        stemmedword = stemmer.stem(word)\n",
        "        stemmed.append(stemmedword)\n",
        "    return stemmed\n",
        "\n",
        "def normalize(terms):\n",
        "    terms = terms.lower()\n",
        "    terms = remove_stopwords(terms)\n",
        "    word_delimiters = u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013 ]'\n",
        "    term_list = re.split(word_delimiters, terms)\n",
        "    trimmed = [x.rstrip() for x in term_list]\n",
        "    stemmed = stem_list(trimmed)\n",
        "    space = ' '\n",
        "    normed = space.join(stemmed)\n",
        "    normed = normed.replace('  ', ' ')\n",
        "    return normed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsuGlOW8h-bd"
      },
      "source": [
        "In the code block above, we have functions to remove stopwords, stem, and normalize the text (remove special characters and trim white space). Apply the normalize function to every yelp review and assign the normalized text to a new column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BDWbek-Yh-be"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tywlufjyh-bf"
      },
      "source": [
        "Next, use the one hot function for text encoding and encode the normalized text. Determine the vocabulary size to perform the encoding."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWKnOSd_h-bg"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnRMy2S0h-bh"
      },
      "source": [
        "Convert the encoded sequences into a numpy array and make sure all reviews are the same length using the `pad_sequences` function in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk24Behch-bi"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIclszsJh-bj"
      },
      "source": [
        "Split the data into train and test. Use 20% for test. The sentiment column should be used as the target variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlaZSo-Yh-bk"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAsqg_REh-bm"
      },
      "source": [
        "Create a sequential model. The model should contain an embedding layer with input dim that is the size of the largest encoding in the vocabulary. The output dim should be 100, the input length is the number of columns in the training data. \n",
        "After the embedding layer, add a SimpleRNN layer with unit size 32, a dense layer of size 8 and a dense output layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssougKQUh-bm"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjgqWnioh-bo"
      },
      "source": [
        "Compile using the optimizer of your choice, use crossentropy for your loss function. Fit the model using a batch size of 128 and 50 epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewNbf3fKh-bo"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jk88z8R8h-bq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}