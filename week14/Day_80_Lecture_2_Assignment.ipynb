{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "colab": {
      "name": "Day 80 Lecture 2 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkd31of4jh35"
      },
      "source": [
        "## Day 85 Lecture 1 Assignment\n",
        "\n",
        "In this assignment, we will learn how to use the other layers to improve our model performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtUi79zbjh36"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZ2DH6Ndjh39"
      },
      "source": [
        "We will explore a dataset containing information about twitter users and will detect whether or not the user is a bot."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cj8bsG8Ijh3-"
      },
      "source": [
        "twitter = pd.read_csv('https://tf-assets-prod.s3.amazonaws.com/tf-curric/data-science/training_data_2_csv_UTF.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MhRde7jh4A",
        "outputId": "8db3e96a-04f6-4353-c8b3-998334d40d35"
      },
      "source": [
        "twitter.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>id_str</th>\n",
              "      <th>screen_name</th>\n",
              "      <th>location</th>\n",
              "      <th>description</th>\n",
              "      <th>url</th>\n",
              "      <th>followers_count</th>\n",
              "      <th>friends_count</th>\n",
              "      <th>listed_count</th>\n",
              "      <th>created_at</th>\n",
              "      <th>favourites_count</th>\n",
              "      <th>verified</th>\n",
              "      <th>statuses_count</th>\n",
              "      <th>lang</th>\n",
              "      <th>status</th>\n",
              "      <th>default_profile</th>\n",
              "      <th>default_profile_image</th>\n",
              "      <th>has_extended_profile</th>\n",
              "      <th>name</th>\n",
              "      <th>bot</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.160000e+17</td>\n",
              "      <td>\"815745789754417152\"</td>\n",
              "      <td>\"HoustonPokeMap\"</td>\n",
              "      <td>\"Houston, TX\"</td>\n",
              "      <td>\"Rare and strong PokŽmon in Houston, TX. See m...</td>\n",
              "      <td>\"https://t.co/dnWuDbFRkt\"</td>\n",
              "      <td>1291</td>\n",
              "      <td>0</td>\n",
              "      <td>10</td>\n",
              "      <td>\"Mon Jan 02 02:25:26 +0000 2017\"</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>78554</td>\n",
              "      <td>\"en\"</td>\n",
              "      <td>{\\r      \"created_at\": \"Sun Mar 12 15:44:04 +0...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>\"Houston PokŽ Alert\"</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.843621e+09</td>\n",
              "      <td>4843621225</td>\n",
              "      <td>kernyeahx</td>\n",
              "      <td>Templeville town, MD, USA</td>\n",
              "      <td>From late 2014 Socium Marketplace will make sh...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "      <td>349</td>\n",
              "      <td>0</td>\n",
              "      <td>2/1/2016 7:37</td>\n",
              "      <td>38</td>\n",
              "      <td>False</td>\n",
              "      <td>31</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Keri Nelson</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.303727e+09</td>\n",
              "      <td>4303727112</td>\n",
              "      <td>mattlieberisbot</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Inspired by the smart, funny folks at @replyal...</td>\n",
              "      <td>https://t.co/P1e1o0m4KC</td>\n",
              "      <td>1086</td>\n",
              "      <td>0</td>\n",
              "      <td>14</td>\n",
              "      <td>Fri Nov 20 18:53:22 +0000 2015</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>713</td>\n",
              "      <td>en</td>\n",
              "      <td>{'retweeted': False, 'is_quote_status': False,...</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>Matt Lieber Is Bot</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.063139e+09</td>\n",
              "      <td>3063139353</td>\n",
              "      <td>sc_papers</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>33</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>2/25/2015 20:11</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>676</td>\n",
              "      <td>en</td>\n",
              "      <td>Construction of human anti-tetanus single-chai...</td>\n",
              "      <td>True</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>single cell papers</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.955142e+09</td>\n",
              "      <td>2955142070</td>\n",
              "      <td>lucarivera16</td>\n",
              "      <td>Dublin, United States</td>\n",
              "      <td>Inspiring cooks everywhere since 1956.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>11</td>\n",
              "      <td>745</td>\n",
              "      <td>0</td>\n",
              "      <td>1/1/2015 17:44</td>\n",
              "      <td>146</td>\n",
              "      <td>False</td>\n",
              "      <td>185</td>\n",
              "      <td>en</td>\n",
              "      <td>NaN</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>lucarivera16</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "             id                id_str       screen_name  \\\n",
              "0  8.160000e+17  \"815745789754417152\"  \"HoustonPokeMap\"   \n",
              "1  4.843621e+09            4843621225         kernyeahx   \n",
              "2  4.303727e+09            4303727112   mattlieberisbot   \n",
              "3  3.063139e+09            3063139353         sc_papers   \n",
              "4  2.955142e+09            2955142070      lucarivera16   \n",
              "\n",
              "                    location  \\\n",
              "0              \"Houston, TX\"   \n",
              "1  Templeville town, MD, USA   \n",
              "2                        NaN   \n",
              "3                        NaN   \n",
              "4      Dublin, United States   \n",
              "\n",
              "                                         description  \\\n",
              "0  \"Rare and strong PokŽmon in Houston, TX. See m...   \n",
              "1  From late 2014 Socium Marketplace will make sh...   \n",
              "2  Inspired by the smart, funny folks at @replyal...   \n",
              "3                                                NaN   \n",
              "4             Inspiring cooks everywhere since 1956.   \n",
              "\n",
              "                         url  followers_count  friends_count  listed_count  \\\n",
              "0  \"https://t.co/dnWuDbFRkt\"             1291              0            10   \n",
              "1                        NaN                1            349             0   \n",
              "2    https://t.co/P1e1o0m4KC             1086              0            14   \n",
              "3                        NaN               33              0             8   \n",
              "4                        NaN               11            745             0   \n",
              "\n",
              "                         created_at  favourites_count  verified  \\\n",
              "0  \"Mon Jan 02 02:25:26 +0000 2017\"                 0     False   \n",
              "1                     2/1/2016 7:37                38     False   \n",
              "2    Fri Nov 20 18:53:22 +0000 2015                 0     False   \n",
              "3                   2/25/2015 20:11                 0     False   \n",
              "4                    1/1/2015 17:44               146     False   \n",
              "\n",
              "   statuses_count  lang                                             status  \\\n",
              "0           78554  \"en\"  {\\r      \"created_at\": \"Sun Mar 12 15:44:04 +0...   \n",
              "1              31    en                                                NaN   \n",
              "2             713    en  {'retweeted': False, 'is_quote_status': False,...   \n",
              "3             676    en  Construction of human anti-tetanus single-chai...   \n",
              "4             185    en                                                NaN   \n",
              "\n",
              "   default_profile  default_profile_image has_extended_profile  \\\n",
              "0             True                  False                False   \n",
              "1             True                  False                False   \n",
              "2             True                  False                False   \n",
              "3             True                   True                False   \n",
              "4            False                  False                False   \n",
              "\n",
              "                   name  bot  \n",
              "0  \"Houston PokŽ Alert\"    1  \n",
              "1           Keri Nelson    1  \n",
              "2    Matt Lieber Is Bot    1  \n",
              "3    single cell papers    1  \n",
              "4          lucarivera16    1  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zV0g49h5jh4D",
        "outputId": "ac6daffd-ec1b-4195-8956-c7862ef1e2e4"
      },
      "source": [
        "twitter.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2797, 20)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dsXMG4zajh4E"
      },
      "source": [
        "Start by getting rid of all columns that are not useful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vE1y_FXejh4F"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQBDJyWPjh4H"
      },
      "source": [
        "Next, get rid of all columns that contain more than 30% missing data. After that, remove all rows containing at least one missing observation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiljewDCjh4H"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_E-SvW3wjh4J"
      },
      "source": [
        "Now we will use our embedding functions from a previous assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeZkChWxjh4K"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def remove_stopwords(input_text):\n",
        "        stopwords_list = stopwords.words('english')\n",
        "        # Some words which might indicate a certain sentiment are kept via a whitelist\n",
        "        whitelist = [\"n't\", \"not\", \"no\"]\n",
        "        words = input_text.split() \n",
        "        clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
        "        return \" \".join(clean_words)       \n",
        "\n",
        "def stem_list(word_list):\n",
        "    stemmed = []\n",
        "    for word in word_list:\n",
        "        stemmedword = stemmer.stem(word)\n",
        "        stemmed.append(stemmedword)\n",
        "    return stemmed\n",
        "\n",
        "def normalize(terms):\n",
        "    terms = terms.lower()\n",
        "    terms = remove_stopwords(terms)\n",
        "    word_delimiters = u'[\\\\[\\\\]\\n.!?,;:\\t\\\\-\\\\\"\\\\(\\\\)\\\\\\'\\u2019\\u2013 ]'\n",
        "    term_list = re.split(word_delimiters, terms)\n",
        "    trimmed = [x.rstrip() for x in term_list]\n",
        "    stemmed = stem_list(trimmed)\n",
        "    space = ' '\n",
        "    normed = space.join(stemmed)\n",
        "    normed = normed.replace('  ', ' ')\n",
        "    return normed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--dr_BGJjh4L"
      },
      "source": [
        "We will create two branches, one branch will process the text data in the description and the other will process all other columns. First, create a numpy array with the encoded data from the description column. Normalize each description, one hot encode the text, pad the row and create a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx5FsY_8jh4M"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXQeqH9Ejh4N"
      },
      "source": [
        "Convert all boolean variables to numeric (zero for false and 1 for true)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OtQ7K3ojh4O"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm8vuH43jh4Q"
      },
      "source": [
        "Create dummy variables out of the relevant object columns. Take caution when converting columns that may incorrectly classified as object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcb0fb4_jh4Q"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OM1NT0I6jh4S"
      },
      "source": [
        "Min max scale the data decribing each user (do not min max scale the word embeddings)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWjM6c2fjh4T"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqnpUBKqjh4V"
      },
      "source": [
        "Now we'll create the two branches. Create a model for the numeric data that consists of 3 dense layers. An input layer and two hidden layers of size 32."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qazo_4w8jh4V"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yRP-ou9Ejh4X"
      },
      "source": [
        "Create the second branch of the model using the encoded words. This branch will consist of 4 layers: An input layer, an embedding layer returning data of dimension 100, an LSTM layer of unit size 32 and a dense layer of unit size 32. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1bLnn2Sjh4Y"
      },
      "source": [
        "# Answer below:\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbx549Vljh4Z"
      },
      "source": [
        "Merge the two models using the `concatenate` function (merge the two final dense layers in each branch) and create an output dense layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6QrfrYijh4a"
      },
      "source": [
        "# Answer below:\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz0-pz-Cjh4b"
      },
      "source": [
        "Create a model using the two inputs and the single output and print the summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZJtI6wAjh4c"
      },
      "source": [
        "# Answer below: \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHk3RhaSjh4e"
      },
      "source": [
        "Compile and fit the model using the appropriate optimizer, loss, and metrics. Train the model for 10 epochs with a batch size of 128."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ceKDMXNjh4e"
      },
      "source": [
        "# Answer below:\n",
        "    \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhlBHQVnjh4h"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}