{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ilinechart(df, x, y, groups=None, title=''):\n",
    "    fig = px.line(df, x=x, y=y, color=groups, title=title, \n",
    "                  template='none').update(layout=dict(title=dict(x=0.5)))\n",
    "    \n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linechart(df, x, length=8, width=15, title=\"\"):\n",
    "    if df.index.name != x:\n",
    "        df = df.set_index(x)\n",
    "\n",
    "    ax = df.plot(figsize=(width,length), cmap=\"Set2\")\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
    "              fancybox=True, shadow=True, ncol=4)\n",
    "    \n",
    "    plt.title(title + \"\\n\", fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iscatter(df, x, y, color=None, size=None, title=''):\n",
    "    fig = px.scatter(df, x=x, y=y, color=color, size=size, \n",
    "                     title=title, template='none')\n",
    "    \n",
    "    fig.update_traces(marker_line_color='black', \n",
    "                  marker_line_width=1)\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans(df, clusters=2):\n",
    "    model = KMeans(n_clusters=clusters, random_state=42)\n",
    "    clusters = model.fit_predict(df)\n",
    "    results = df.copy()\n",
    "    results['Cluster'] = clusters\n",
    "    \n",
    "    cluster_size = results.groupby(['Cluster']).size().reset_index()\n",
    "    cluster_size.columns = ['Cluster', 'Count']\n",
    "    cluster_means = results.groupby(['Cluster'], as_index=False).mean()\n",
    "    summary = pd.merge(cluster_size, cluster_means, on='Cluster')\n",
    "    \n",
    "    return results, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_stats(corpus):\n",
    "    print(\"Corpus Statistics\")\n",
    "    print(\"Number of documents: \" + str(len(corpus.fileids())))\n",
    "    print(\"Number of paragraphs: \" + str(len(corpus.paras())))\n",
    "    print(\"Number of sentences: \" + str(len(corpus.sents())))\n",
    "    print(\"Number of words: \" + str(len(corpus.words())))\n",
    "    print(\"Vocabulary: \" + str(len(set(w.lower() for w in corpus.words()))))\n",
    "    print(\"Avg chars per word: \" + str(round(len(corpus.raw())/len(corpus.words()),1)))\n",
    "    print(\"Avg words per sentence: \" + str(round(len(corpus.words())/len(corpus.sents()),1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fe584783e7a3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m documents = [TaggedDocument(doc, [i]) \n\u001b[1;32m----> 4\u001b[1;33m              for i, doc in enumerate(preprocessed)]\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed' is not defined"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) \n",
    "             for i, doc in enumerate(preprocessed)]\n",
    "\n",
    "model = Doc2Vec(documents)\n",
    "\n",
    "doc2vec = pd.DataFrame([[document]+list(model[document]) \n",
    "                        for document in range(len(docs))]).drop(0, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_stats(doc):\n",
    "    sents = sent_tokenize(doc)\n",
    "    tokens = word_tokenize(doc)\n",
    "    words = [token.lower() for token in tokens \n",
    "             if not token.lower() in stopwords.words('english')\n",
    "             if not token in string.punctuation]\n",
    "\n",
    "    num_sents = len(sents)\n",
    "    num_tokens = len(tokens)\n",
    "    num_words = len(words)\n",
    "    vocab = len(set(words))\n",
    "    characters = sum([len(word) for word in words])\n",
    "    \n",
    "    spacy_doc = nlp(doc)\n",
    "    remove = ['DATE', 'TIME', 'PERCENT', 'MONEY', 'QUANTITY', \n",
    "              'ORDINAL', 'CARDINAL']\n",
    "    entities = [entity.text for entity in spacy_doc.ents \n",
    "                if not entity.label_ in remove]\n",
    "\n",
    "    num_entities = len(set(entities))\n",
    "    words_sent = num_words / num_sents\n",
    "    char_word = characters / num_words\n",
    "    lex_div = vocab / num_words\n",
    "    \n",
    "    stats = [num_sents, num_tokens, num_words, vocab, num_entities, \n",
    "             words_sent, char_word, lex_div]\n",
    "\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f3fbaebc649f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m cleaned = [token.lower() for token in word_tokenize(doc) \n\u001b[0m\u001b[0;32m      2\u001b[0m            \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m            if token.isalpha()==True]\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "cleaned = [token.lower() for token in word_tokenize(doc) \n",
    "           if not token.lower() in stopwords.words('english') \n",
    "           if token.isalpha()==True]\n",
    "\n",
    "fdist = FreqDist(cleaned)\n",
    "fdist_df = pd.DataFrame.from_dict(fdist, orient='index').reset_index()\n",
    "fdist_df.columns = ['Term', 'Freq']\n",
    "ordered = fdist_df.sort_values('Freq', ascending=False)\n",
    "filtered = ordered[ordered['Freq'] > 1]\n",
    "\n",
    "plt.figure(figsize=(14,8))\n",
    "sns.barplot(data=filtered, x='Freq', y=\"Term\")\n",
    "plt.title('Term Frequency Distribution \\n', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_cloud(text, colormap='tab10', background_color='white'):\n",
    "    cloud = WordCloud(width=1600, height=800, stopwords=STOPWORDS,\n",
    "                      colormap=colormap, \n",
    "                      background_color=background_color).generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(12,10))\n",
    "    plt.imshow(cloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
